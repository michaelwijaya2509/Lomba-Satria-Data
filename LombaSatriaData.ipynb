{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "6Ah2bte9ovrs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7QO9fMbm6j4",
        "outputId": "1a2a10b9-d0d6-4b3b-bd4a-78f7ee3970fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data"
      ],
      "metadata": {
        "id": "NmHxojvSy9RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = r\"/content/gdrive/MyDrive/Lomba Satria Data/dataset_penyisihan_bdc_2024.xlsx\"\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Assuming the Excel file has columns named 'text' and 'label'\n",
        "X = data['text']\n",
        "y = data['label']\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD69rUpszAzV",
        "outputId": "8f8ada50-8284-481c-bdbc-d67e6ff2eff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text             label\n",
            "0  Kunjungan Prabowo ini untuk meresmikan dan men...  Sumber Daya Alam\n",
            "1  RT Anies dapat tepuk tangan meriah saat jadi R...           Politik\n",
            "2  @CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...         Demografi\n",
            "3  RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...           Politik\n",
            "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...           Politik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pembersihan data**"
      ],
      "metadata": {
        "id": "VZOGMrSN4T0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = WordNetLemmatizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "documents = []\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Convert to string\n",
        "    document = str(X[sen])\n",
        "\n",
        "    # Remove URLs\n",
        "    document = re.sub(r'http\\S+|www\\S+|https\\S+', '', document, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove sequences of characters that don't form words (random strings)\n",
        "    document = re.sub(r'\\b\\w{10,}\\b', '', document)\n",
        "\n",
        "    # Remove specific string \"[RE coldthem]\"\n",
        "    document = re.sub(r'\\[RE coldthem\\]', '', document)\n",
        "\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', document)\n",
        "\n",
        "    # Remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "    # Substitute multiple spaces with a single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "    # Remove the word \"RT\"\n",
        "    document = re.sub(r'\\bRT\\b', '', document)\n",
        "\n",
        "    # Remove prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "    document = re.sub(r'(\\d+)([a-zA-Z]+)(\\d+)', lambda m: m.group(2), document)\n",
        "\n",
        "    document = re.sub(r'[^A-Za-z0-9\\s]', '', document)\n",
        "\n",
        "    document = re.sub(r'\\[.*?\\]', '', document)\n",
        "\n",
        "    document = re.sub(r'02', 'prabowo', document)\n",
        "\n",
        "    document = re.sub(r'01', 'anies', document)\n",
        "\n",
        "    document = re.sub(r'02', 'ganjar', document)\n",
        "\n",
        "\n",
        "    wordsToRemove = ['dan', 'serta', 'lagipula', 'setelah', 'sejak', 'selanjutnya', 'tetapi', 'melainkan',\n",
        "                    'sedangkan', 'atau', 'ataupun', 'maupun', 'untuk', 'agar', 'supaya', 'sebab', 'karena',\n",
        "                    'sehingga', 'sampai', 'akibatnya', 'lalu', 'kemudian', 'jika', 'kalau', 'jikalau', 'apabila',\n",
        "                    'walaupun', 'maupun', 'meskipoun', 'biarpun', 'seperti', 'sebagai', 'bagaikan', 'biar',\n",
        "                    'biarpun', 'bahkan', 'yaitu', 'yakni', 'kecuali', 'selain', 'goblok', 'tolol', 'jancuk',\n",
        "                    'rt', 'tengil', 're', 'detikbali', 'fuck', 'lo', 'lu', 'letoy', 'cengeng', 'n', 'o', 'pantat',\n",
        "                    'gue',]\n",
        "\n",
        "    document = ' '.join(word for word in document.split() if word not in wordsToRemove)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    document = document.lower()\n",
        "\n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    document = ' '.join(word for word in document.split() if word not in wordsToRemove)\n",
        "\n",
        "    documents.append(document)\n",
        "\n",
        "# Display the first few processed documents to verify\n",
        "for i in range(10):  # Change the number as needed to view more or fewer documents\n",
        "    print(f\"Document {i+1}: {documents[i]}\")"
      ],
      "metadata": {
        "id": "Gg0P5qXo7XlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a83fdde-0f33-40c6-e535-4d2477a1e604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: kunjungan prabowo ini proyek bantuan air bersih di lima titik prabowo subianto\n",
            "Document 2: anies dapat tepuk tangan meriah saat jadi rektor mata kuliah anti korupsi memutus mata rantai korupsi ekowboy2\n",
            "Document 3: emng bener sih pendukung anies ada yg begitu jg dg pendukung prabowo hnya sj menurut pak ridwan kamil skemanya terbalik klo anies mayoritas menengah atas artinya ada jg rendah yg milih\n",
            "Document 4: sewaktu anies bersikap kriti ke kinerja pak prabowo dianggap engga sopan dianggap kurang orang tua giliran skrg gibran yg sok kriti malah dianggap kriti kera apakah ini tidak standar ganda\n",
            "Document 5: anies baswedan harap asn termasuk tni polri pegang sumpahnya dalam pemilu\n",
            "Document 6: duh jangan pak lurah denger nih di acara hajatan rakyat puluhan ribu warga di kendal serukan ganjar kehadiran jdlc menjadi magnet bagi puluhan ribu warga datang hajatan rakyat ganjar mahfud besarnya warga menjadi bukti bahwa jawa tengah tetap menjadi kandang banteng dsyantie\n",
            "Document 7: prabowo minta kemenhan tim satgas air unhan kaji bantuan air di sukabumi dekade08 prabowo gibran\n",
            "Document 8: anies ya allah orang zalim dapat kejahatan dari orang zalim lainnya\n",
            "Document 9: abah ciazcq itu bapa kami bapa yg berikan perhatian anak anak papua jadi pintar pintar gerakan indonesia mengajar tak bisa di lupakan sama warga papua kami ingin bapa anies jadi presiden\n",
            "Document 10: bawaslu dimaki sama warga gara mencopot spanduk ganjar mahfud sementara spanduk capres lain tidak dicopot bawaslu rasa tim sukses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization\n"
      ],
      "metadata": {
        "id": "xhkuUnlxGSfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text data to numerical data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "# Convert labels to numerical values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "cNf8NzE4GV6a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch data preparation"
      ],
      "metadata": {
        "id": "ujRWpwmhSD_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = torch.tensor(texts, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Create dataset\n",
        "dataset = TextDataset(X_tfidf, y_encoded)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "XrhwtttLSIWc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Neural Network Model"
      ],
      "metadata": {
        "id": "tb0cYG-GSYY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set input dimension (number of features from TF-IDF), hidden dimension, and output dimension (number of classes)\n",
        "input_dim = X_tfidf.shape[1]\n",
        "hidden_dim = 100\n",
        "output_dim = len(encoder.classes_)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "V0ZyG_gXSX7r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "iXKPRz1BSfom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFvswrn8SjUR",
        "outputId": "2ee3d08f-e390-4a43-d2fe-6854077592ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.2104, Accuracy: 0.6080\n",
            "Epoch 2/10, Loss: 0.9123, Accuracy: 0.7220\n",
            "Epoch 3/10, Loss: 0.7940, Accuracy: 0.7550\n",
            "Epoch 4/10, Loss: 0.7742, Accuracy: 0.7630\n",
            "Epoch 5/10, Loss: 0.7806, Accuracy: 0.7620\n",
            "Epoch 6/10, Loss: 0.7980, Accuracy: 0.7590\n",
            "Epoch 7/10, Loss: 0.8360, Accuracy: 0.7600\n",
            "Epoch 8/10, Loss: 0.8662, Accuracy: 0.7520\n",
            "Epoch 9/10, Loss: 0.8981, Accuracy: 0.7540\n",
            "Epoch 10/10, Loss: 0.9295, Accuracy: 0.7450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Eval"
      ],
      "metadata": {
        "id": "pk8YHqelSw7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in val_loader:\n",
        "        outputs = model(texts)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.tolist())\n",
        "        all_predictions.extend(predicted.tolist())\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(all_labels, all_predictions, target_names=encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PExErjfSygC",
        "outputId": "c8dd596f-09bd-4cb1-f392-ea369fdf6732"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         precision    recall  f1-score   support\n",
            "\n",
            "              Demografi       0.67      0.17      0.27        12\n",
            "                Ekonomi       0.72      0.82      0.77        60\n",
            "               Geografi       0.00      0.00      0.00         5\n",
            "               Ideologi       0.62      0.61      0.61        79\n",
            "Pertahanan dan Keamanan       0.72      0.63      0.67        70\n",
            "                Politik       0.79      0.87      0.83       608\n",
            "          Sosial Budaya       0.59      0.45      0.51       122\n",
            "       Sumber Daya Alam       0.59      0.43      0.50        44\n",
            "\n",
            "               accuracy                           0.74      1000\n",
            "              macro avg       0.59      0.50      0.52      1000\n",
            "           weighted avg       0.73      0.74      0.73      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FInal"
      ],
      "metadata": {
        "id": "sO-Nj6lzTQ9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Predict labels for the entire dataset\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for texts in torch.tensor(X_tfidf, dtype=torch.float32):\n",
        "        outputs = model(texts.unsqueeze(0))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_predictions.append(predicted.item())\n",
        "\n",
        "# Add predictions to the DataFrame\n",
        "data['predicted_label'] = encoder.inverse_transform(all_predictions)\n"
      ],
      "metadata": {
        "id": "1tbQbtlbTSXJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_encoded, all_predictions)\n",
        "print(f'Overall Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqwizRW3TWE4",
        "outputId": "0230b109-58c1-43ad-a924-6eeff77aa085"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.9312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_path = '/content/gdrive/MyDrive/outputtrainingfile.xlsx'\n",
        "data.to_excel(output_file_path, index=False)\n",
        "print(f\"Updated dataset with predictions saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQHqQK7bTcPw",
        "outputId": "716ab764-4307-4e43-da44-0d089253eb21"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated dataset with predictions saved to /content/gdrive/MyDrive/outputtrainingfile.xlsx\n"
          ]
        }
      ]
    }
  ]
}